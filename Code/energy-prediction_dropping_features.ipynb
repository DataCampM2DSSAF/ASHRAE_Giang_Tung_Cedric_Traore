{"cells":[{"metadata":{},"cell_type":"markdown","source":"Improvement from the first submission:\n- Preprocessing:\n    - Missing data\n        Air_temperature: fill with values from previous days\n        Fill others variables\n    - Timestamps alignment\n    - Target variable : \n        - Remove outliers\n        - Log1p transformation\n- Feature engineering:\n    - Study the correlation between variables\n    - Regrouping features\n- Predict on subsets of the test set (due to the limitation of memory)"},{"metadata":{},"cell_type":"markdown","source":"<a href='#1'>1. Data's Overview</a>\n\n<a href='#2'>2. Preprocessing</a>\n\n<a href='#3'>3. Feature engineering</a>\n\n<a href='#4'>4. Simple model</a>"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # basic plotting\nimport seaborn as sns # for prettier plots\nimport datetime # manipulating date formats\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n","execution_count":48,"outputs":[{"output_type":"stream","text":"/kaggle/input/ashrae-energy-prediction/sample_submission.csv\n/kaggle/input/ashrae-energy-prediction/building_metadata.csv\n/kaggle/input/ashrae-energy-prediction/weather_train.csv\n/kaggle/input/ashrae-energy-prediction/weather_test.csv\n/kaggle/input/ashrae-energy-prediction/train.csv\n/kaggle/input/ashrae-energy-prediction/test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# <a id='1'>1. Data's Overview</a>"},{"metadata":{},"cell_type":"markdown","source":"## Loading data"},{"metadata":{"trusted":true},"cell_type":"code","source":"building_metadata = pd.read_csv('/kaggle/input/ashrae-energy-prediction/building_metadata.csv')\ntrain = pd.read_csv('/kaggle/input/ashrae-energy-prediction/train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv('/kaggle/input/ashrae-energy-prediction/test.csv', parse_dates=['timestamp'])\nweather_train = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_train.csv', parse_dates=['timestamp'])\nweather_test = pd.read_csv('/kaggle/input/ashrae-energy-prediction/weather_test.csv', parse_dates=['timestamp'])","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Function to reduction memory usage (Source code from Kaggle)\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df","execution_count":50,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data's overview"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"print('Size of train data', train.shape)\nprint('Size of test data', test.shape)\nprint('Size of weather_train data', weather_train.shape)\nprint('Size of weather_test data', weather_test.shape)\nprint('Size of building_metadata data', building_metadata.shape)\"\"\"","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"\"print('Size of train data', train.shape)\\nprint('Size of test data', test.shape)\\nprint('Size of weather_train data', weather_train.shape)\\nprint('Size of weather_test data', weather_test.shape)\\nprint('Size of building_metadata data', building_metadata.shape)\""},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train.head()","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#weather_train.head()","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building_metadata.head()","execution_count":54,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def missing(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percentage = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n    missing_data  = pd.concat([total, percentage], axis=1, keys=['Total', 'Percentage'])\n    return missing_data.head(data.shape[1])","execution_count":55,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing(weather_train)","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing(weather_test)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing(building_metadata)","execution_count":58,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='2'>2. Preprocessing</a>"},{"metadata":{},"cell_type":"markdown","source":"## a. Missing data"},{"metadata":{},"cell_type":"markdown","source":"### Air_temperature\n\nThere are only 55 missing values in the weather_train data set.\n\nWe fill the missing data with the value from the closest previous day without na, at the same time of the day, of the same site (there's no NA from the same hour in two consecutive days of the same site)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\ndef fill_air_temp(data):\n    \n    '''Function for filling Na data in air_temperature variable'''\n    \n    na_index = data[data['air_temperature'].isnull()].index\n    temp_key = ['site_id', 'timestamp', 'air_temperature']\n    \n    for i in na_index:\n        site_na = data.loc[i, 'site_id']   \n        for j in range(5):\n            time_na = data.loc[i,'timestamp'] - datetime.timedelta(days = j+1)  \n            ind = data[(data.site_id == site_na) & (data.timestamp == time_na)].index  \n            \n            if math.isnan(data.loc[ind, 'air_temperature']) == False :\n                data.loc[i, 'air_temperature'] = data.loc[ind, 'air_temperature'].values\n                break\n    \n    return data","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train = fill_air_temp(weather_train)\nweather_test = fill_air_temp(weather_test)","execution_count":60,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### floor_count and year_built"},{"metadata":{"trusted":true},"cell_type":"code","source":"#According to the above table, 53.41% of year_built and 75.50% \n#of floor_count are not available\n\n#We fill the floor_count's NA by mode = 1 (repsent 7.52% of floor_count values) \n#and the year_built by the mean of it value\nbuilding_metadata.fillna({'floor_count':1,'year_built': building_metadata['year_built'].mean()}, inplace = True)\nbuilding_metadata['primary_use'] = building_metadata['primary_use'].astype('category')","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Other variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Even the weather data don't change more in the futur and the past, so we make a \n#forward and backward filling\n# First: transformation of the timestamp into a datetime object \n# Second: sorting by site id then timestamp\n\nweather_train = weather_train.sort_values(by=['site_id', 'timestamp']) \nweather_train.fillna(method = 'bfill', inplace=True, limit = 12) #backfill up to 12 hours\nweather_train.fillna(method = 'ffill', inplace = True, limit = 12) #forward fill the missing data up to 12 hours\n\n\n## Weather test\nweather_test = weather_test.sort_values(by=['site_id', 'timestamp']) \nweather_test.fillna(method = 'bfill', inplace=True, limit = 12)\nweather_test.fillna(method = 'ffill', inplace=True, limit = 12)\n\n","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#sns.heatmap(weather_train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\n#sns.heatmap(weather_test.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')","execution_count":63,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train data\nmissing_cols = [col for col in weather_train.columns if weather_train[col].isna().any()] \nfill_lib = weather_train.groupby('site_id')[missing_cols].transform('mean')\n#stores the mean of each feature for each site id\nweather_train.fillna(fill_lib, inplace=True) #for each feature with missing \n#values, fill the missing entry with the mean for that site\n\n#Test data\nmissing_cols = [col for col in weather_test.columns if weather_test[col].isna().any()] \nfill_lib = weather_test.groupby('site_id')[missing_cols].transform('mean')\n#stores the mean of each feature for each site id\nweather_test.fillna(fill_lib, inplace=True)\ndel missing_cols, fill_lib\n\nweather_train.isna().sum()\n#weather_test.isna().sum()","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"site_id               0\ntimestamp             0\nair_temperature       0\ncloud_coverage        0\ndew_temperature       0\nprecip_depth_1_hr     0\nsea_level_pressure    0\nwind_direction        0\nwind_speed            0\ndtype: int64"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## b. Align the timestamp"},{"metadata":{},"cell_type":"markdown","source":"The timestamps of the weather data and the train/test data are different: those in the weather data is not in the site's local time. So there's need an alignment before merging these dataset by timestamps."},{"metadata":{},"cell_type":"markdown","source":"To see this problem, we plot the air temperature (from weather data) and energy consumption (from train data) by site and hour.\nWe assume that highest air temperature should appear at around 14:00."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_by_site_by_hour(data, column) :\n    '''Plot a variable by site and hour'''\n    \n    plot_key = ['site_id', 'timestamp']\n    col_to_plot = data[plot_key + [column]].copy()\n    col_to_plot['hour'] = col_to_plot['timestamp'].dt.hour\n    \n    c = 1\n    plt.figure(figsize=(25, 15))\n    for site_id, data_by_site in col_to_plot.groupby('site_id'):\n        by_site_by_hour = data_by_site.groupby('hour').mean()\n        ax = plt.subplot(4, 4, c)\n        plt.plot(by_site_by_hour.index,by_site_by_hour[column],'xb-')\n        ax.set_title('site: '+str(site_id))\n        c += 1\n    return plt.show()","execution_count":65,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather = pd.concat([weather_train,weather_test],ignore_index=True)","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_by_site_by_hour(weather,'air_temperature')","execution_count":67,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see that the peak temperature for most of these site are not around 14:00, some even at night, which doesn't make sense."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Create dataframe having site_id and meter from train and metadata datasets\nbuilding_site_dict = dict(zip(building_metadata['building_id'], building_metadata['site_id']))\nsite_meter = train[['building_id', 'meter', 'timestamp', 'meter_reading']].copy()\nsite_meter['site_id'] = site_meter.building_id.map(building_site_dict)\ndel site_meter['building_id']\n\n#Dataframe with site_id and electrical consumption\nsite_elec = site_meter[site_meter.meter == 0]","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plot_by_site_by_hour(site_elec, 'meter_reading')\ndel building_site_dict, site_meter, site_elec\ngc.collect()","execution_count":69,"outputs":[{"output_type":"execute_result","execution_count":69,"data":{"text/plain":"100"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"The energy consumption of these sites make sense, which mean that the timestamps are more aligned."},{"metadata":{"trusted":true},"cell_type":"code","source":"# calculate ranks of hourly temperatures within date and site_id\nkey = ['site_id', 'timestamp', 'air_temperature']\ntemp = weather.loc[:,key]\ntemp['temp_rank'] = temp.groupby(['site_id', temp.timestamp.dt.date])['air_temperature'].rank('average')\n\n# site_id x mean hour rank of temperature within day\nmean_rank = temp.groupby(['site_id', temp.timestamp.dt.hour])['temp_rank'].mean().unstack(level=1)","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the timestamp alignment gap for each site\ngap = pd.Series(mean_rank.values.argmax(axis=1) - 14)\ngap.index.name = 'site_id'\n\ndef timestamp_align(data):\n    '''Function to align the timestamp'''\n    data['offset'] = data.site_id.map(gap)\n    data['timestamp_aligned'] = (data.timestamp - pd.to_timedelta(data.offset, unit='H'))\n    data['timestamp'] = data['timestamp_aligned']\n    del data['timestamp_aligned']\n    return data","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weather_train = timestamp_align(weather_train)\nweather_test = timestamp_align(weather_test)\ndel weather, temp, gap\ngc.collect()","execution_count":72,"outputs":[{"output_type":"execute_result","execution_count":72,"data":{"text/plain":"77"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## c. Merge data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge = train.merge(building_metadata, on='building_id', how='left', validate='many_to_one')\ntrain_merge = train_merge.merge(weather_train, on=['site_id', 'timestamp'], how='left', validate='many_to_one')\ndel train, weather_train\n\ntest_merge = test.merge(building_metadata, on='building_id', how='left', validate='many_to_one')\ntest_merge = test_merge.merge(weather_test, on=['site_id', 'timestamp'], how='left', validate='many_to_one')\ndel test, weather_test, building_metadata","execution_count":73,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This additionnal missing values appear here because some merging identity (timesamp) of train and test dataset aren't in  weather data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_data\ntrain_merge = train_merge.sort_values(by=['building_id', 'timestamp'])\ntrain_merge.fillna(method = 'ffill', inplace=True)\n\n#test data\ntest_merge = test_merge.sort_values(by=['building_id', 'timestamp'])\ntest_merge.fillna(method = 'ffill', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge = reduce_mem_usage(train_merge)\ntest_merge = reduce_mem_usage(test_merge)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## d. Create new variables"},{"metadata":{},"cell_type":"markdown","source":"With the problem of the form of time series, we can create some useful variables. For example: year, month, date, hour, lagged variables. We will start with the simple ones: year, month, date, hour."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge['month'] = train_merge['timestamp'].dt.month.astype(np.int8)\n#train_merge['year'] = train_merge['timestamp'].dt.year.astype(np.int16)\ntrain_merge['day'] = train_merge['timestamp'].dt.dayofweek.astype(np.int8)\ntrain_merge['week'] = train_merge['timestamp'].dt.weekofyear.astype(np.int8)\ntrain_merge['hour'] = train_merge['timestamp'].dt.hour.astype(np.int8)\n\ntest_merge['month'] = test_merge['timestamp'].dt.month.astype(np.int8)\n#test_merge['year'] = test_merge['timestamp'].dt.year.astype(np.int16)\ntest_merge['day'] = test_merge['timestamp'].dt.dayofweek.astype(np.int8)\ntest_merge['week'] = test_merge['timestamp'].dt.weekofyear.astype(np.int8)\ntest_merge['hour'] = test_merge['timestamp'].dt.hour.astype(np.int8)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## e. Target variable and transformation"},{"metadata":{},"cell_type":"markdown","source":" Our original target variable is meter_reading.\n - Type 0 (electricity) of meter are mesure in kBTU. We need to convert them to kWh, and convert the prediction of this type again to kBTU before submission.\n - Transform the original target variable to obtain a new target variable *log1p(meter_reading)*"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge.loc[train_merge.meter == 0 , 'meter_reading'] = train_merge.loc[train_merge.meter == 0 , 'meter_reading'] * 0.2931","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_merge.meter_reading.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.boxplot(train_merge.meter_reading)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.hist(train_merge.meter_reading, bins = 100)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.hist(np.log1p(train_merge.meter_reading), bins = 100)\n#plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Identify Outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_merge['m/s'] = train_merge['meter_reading']/train_merge['square_feet']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#elec_ms = train_merge[(train_merge.meter==0) & (train_merge.meter_reading > 0)].loc[:,'m/s']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#elec_ms.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create new target variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge['target'] = np.log1p(train_merge['meter_reading'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_0 = train_merge[train_merge.target > 0]\nQ1 = train_0.target.quantile(0.25)\nQ3 = train_0.target.quantile(0.75)\nIQR = Q3 - Q1\ndel train_0\ngc.collect()\ntrain_merge = train_merge[train_merge.target <= Q3 + 1.5 * IQR]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='3'>3. Feature Engineering</a>"},{"metadata":{},"cell_type":"markdown","source":"Correlation between target variable and other variable"},{"metadata":{"trusted":true},"cell_type":"code","source":"#corr = train_merge.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heat map"},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.subplots(figsize=(20, 20))\n#sns.heatmap(corr, annot = True, mask = np.triu(corr))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"train_merge.loc[:,'primary_use'] = train_merge['primary_use'].replace({'Entertainment/public assembly':'public',\n     'Public services':'service','Lodging/residential':'resident','Healthcare':'resident',\n     'Parking':'parking','Warehouse/storage': 'parking','Manufacturing/industrial': 'manufact',\n     'Retail':'service', 'Services':'service', 'Technology/science':'Office',\n     'Food sales and service':'service','Utility':'service','Religious worship':'public'})\n\ntest_merge.loc[:,'primary_use'] = test_merge['primary_use'].replace({'Entertainment/public assembly':'public',\n     'Public services':'service','Lodging/residential':'resident','Healthcare':'resident',\n     'Parking':'parking','Warehouse/storage': 'parking','Manufacturing/industrial': 'manufact',\n     'Retail':'service', 'Services':'service', 'Technology/science':'Office',\n     'Food sales and service':'service','Utility':'service','Religious worship':'public'})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# <a id='4'>4. Simple model</a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeRegressor\n\ny_train = train_merge['target']\nx_train = train_merge.dropna(axis = 1)\n\nx = pd.get_dummies(x_train[['primary_use']],drop_first=True)\nx_train = pd.concat([x_train, x], axis = 1)\nx_train = x_train.drop(['primary_use','meter_reading','target','timestamp',\"sea_level_pressure\", \"wind_direction\", \"wind_speed\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_test = test_merge.dropna(axis = 1)\nx = pd.get_dummies(x_test[['primary_use']],drop_first=True)\nx_test = pd.concat([x_test, x], axis = 1)\nx_test = x_test.drop(['primary_use','timestamp',\"sea_level_pressure\", \"wind_direction\", \"wind_speed\"], axis = 1)\nx_test = x_test.drop('row_id', axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_merge,x\ngc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tree_model = DecisionTreeRegressor(min_samples_split = 200, min_samples_leaf = 100)\ntree_model = tree_model.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Light Gradient Boost Machine"},{"metadata":{"trusted":true},"cell_type":"code","source":"#import lightgbm as lgb\n#from sklearn.model_selection import train_test_split\n#import random\n#random.seed(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"#X_train, X_val, y_train, y_val = train_test_split(x_train,y_train,test_size=0.2)\nX_train1 = x_train[:int(x_train.shape[0] / 2)]\nX_train2 = x_train[int(x_train.shape[0] / 2):]\ndel x_train\n\ny_train1 = y_train[:int(y_train.shape[0]/2)]\ny_train2 = y_train[int(y_train.shape[0] / 2):]\ndel y_train\n\n#categorical_features = ['building_id', 'site_id', 'meter','month','day','week','hour']\n\nX_train1 = lgb.Dataset(X_train1, label=y_train1,free_raw_data=False)\nX_train2 = lgb.Dataset(X_train2, label=y_train2,free_raw_data=False)\ndel y_train1,y_train2\n\n\nparams = {\n    \"objective\": \"regression\",\n    \"boosting\": \"gbdt\",\n    \"num_leaves\": 40,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.85,\n    \"reg_lambda\": 2,\n    \"metric\": \"rmse\",\n    \"force_col_wise\": True\n}\n\n#Building model with first half and validating on second half:\nmodel1 = lgb.train(params, train_set=X_train1, num_boost_round=1000, valid_sets=[X_train1,X_train2],verbose_eval=200, early_stopping_rounds=200)\n\n#Building model with second half and validating on first half:\nmodel2 = lgb.train(params, train_set=X_train2, num_boost_round=1000, valid_sets=[X_train2,X_train1], verbose_eval=200,early_stopping_rounds=200)\ndel X_train1,X_train2\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratio = int(len(x_test)/10)\ny_pred = np.empty(len(x_test))\nfor i in range(10):\n    #y_pred[i*ratio:(i+1)*ratio] = np.expm1(model1.predict(x_test.iloc[i*ratio:(i+1)*ratio],num_iteration=model1.best_iteration))/2\n    #y_pred[i*ratio:(i+1)*ratio] += np.expm1(model2.predict(x_test.iloc[i*ratio:(i+1)*ratio],num_iteration=model2.best_iteration))/2\n    y_pred[i*ratio:(i+1)*ratio] = np.expm1(tree_model.predict(x_test.iloc[i*ratio:(i+1)*ratio]))\ndel tree_model\ngc.collect()    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred[x_test.meter==0] /= 0.2931\nmy_submission = pd.DataFrame({'row_id': test_merge.row_id, 'meter_reading': y_pred})\nmy_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}